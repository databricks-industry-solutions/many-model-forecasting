{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7b75eaf-a052-4407-b2cc-ec52d57e84cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Many Models Forecasting (MMF)\n",
    "This notebook showcases how to run MMF with local models on multiple univariate time series of weekly resolution. We will use [M4 competition](https://www.sciencedirect.com/science/article/pii/S0169207019301128#sec5) data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2fbe300-3aab-4c6a-8bc2-7593909ff71a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cluster setup\n",
    "\n",
    "We recommend using a cluster with [Databricks Runtime 16.4 LTS for ML](https://docs.databricks.com/en/release-notes/runtime/15.6lts-ml.html). The cluster can be either a single-node or multi-node CPU cluster. MMF leverages [Pandas UDF](https://docs.databricks.com/en/udf/pandas.html) under the hood and utilizes all the available resource. Make sure to set the following Spark configurations before you start your cluster: [`spark.sql.execution.arrow.enabled true`](https://spark.apache.org/docs/3.0.1/sql-pyspark-pandas-with-arrow.html#enabling-for-conversion-tofrom-pandas) and [`spark.sql.adaptive.enabled false`](https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution). You can do this by specifying [Spark configuration](https://docs.databricks.com/en/compute/configure.html#spark-configuration) in the advanced options on the cluster creation page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c032624-bc6b-4ece-9438-a612f591f7b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install and import packages\n",
    "Check out [requirements.txt](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/requirements.txt) if you're interested in the libraries we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c3d3b93-d464-4f27-ac9c-254baac43904",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install the necessary libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ../../requirements.txt --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0321fde-6c11-40f5-8cc8-ac5da9c6cf06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = spark._jvm.org.apache.log4j\n",
    "logging.getLogger(\"py4j.java_gateway\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j.clientserver\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93abc2a3-1dbe-40c3-bb68-5c0b84b173e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "from datasetsforecast.m4 import M4\n",
    "from mmf_sa import run_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a7be081-3819-43a7-a4ac-fc0e2be2df8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare data \n",
    "We are using [`datasetsforecast`](https://github.com/Nixtla/datasetsforecast/tree/main/) package to download M4 data. M4 dataset contains a set of time series which we use for testing MMF. Below we have written a number of custome functions to convert M4 time series to an expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f33b51ad-6c47-4288-8455-e1f450f23ab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Number of time series\n",
    "n = 100\n",
    "\n",
    "\n",
    "def create_m4_weekly():\n",
    "    y_df, _, _ = M4.load(directory=str(pathlib.Path.home()), group=\"Weekly\")\n",
    "    _ids = [f\"W{i}\" for i in range(1, n)]\n",
    "    y_df = (\n",
    "        y_df.groupby(\"unique_id\")\n",
    "        .filter(lambda x: x.unique_id.iloc[0] in _ids)\n",
    "        .groupby(\"unique_id\")\n",
    "        .apply(transform_group)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return y_df\n",
    "\n",
    "\n",
    "def transform_group(df):\n",
    "    unique_id = df.unique_id.iloc[0]\n",
    "    if len(df) > 260:\n",
    "        df = df.iloc[-260:]\n",
    "    _start = pd.Timestamp(\"2020-01-01\")\n",
    "    _end = _start + pd.DateOffset(days=int(7*len(df)))\n",
    "    date_idx = pd.date_range(start=_start, end=_end, freq=\"W\", name=\"ds\")\n",
    "    res_df = pd.DataFrame(data=[], index=date_idx).reset_index()\n",
    "    res_df[\"unique_id\"] = unique_id\n",
    "    res_df[\"y\"] = df.y.values\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58b31423-faa5-477a-b1c5-fea877589f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We are going to save this data in a delta lake table. Provide catalog and database names where you want to store the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c95f94a9-5d95-4231-9e7d-dfb8b1d58130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"mmf\" # Name of the catalog we use to manage our assets\n",
    "db = \"m4\" # Name of the schema we use to manage our assets (e.g. datasets)\n",
    "user = spark.sql('select current_user() as user').collect()[0]['user'] # User email address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d863678b-7ce1-4831-abfe-b78888564f30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Making sure that the catalog and the schema exist\n",
    "_ = spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "_ = spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{db}\")\n",
    "\n",
    "(\n",
    "    spark.createDataFrame(create_m4_weekly())\n",
    "    .write.format(\"delta\").mode(\"overwrite\")\n",
    "    .saveAsTable(f\"{catalog}.{db}.m4_weekly_train\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5da4fc14-54f4-42d2-bb0d-e8252f2c3b24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's take a peak at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "998aee27-9bfd-4d3b-92dc-d047deffdf8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  spark.sql(f\"select * from {catalog}.{db}.m4_weekly_train where unique_id in ('W1', 'W2', 'W3', 'W4', 'W5') order by unique_id, ds\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29247004-fec5-45c9-b3b9-3972a040638c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If the number of time series is larger than the number of total cores, we set `spark.sql.shuffle.partitions` to the number of cores (can also be a multiple) so that we don't under-utilize the resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a2978cb-a760-40d3-bd7a-312017f885ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if n > sc.defaultParallelism:\n",
    "    sqlContext.setConf(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd4a4bc-0c66-4469-a1d2-8914b3984aa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Models\n",
    "Let's configure a list of models we are going to apply to our time series for evaluation and forecasting. A comprehensive list of all supported models is available in [mmf_sa/models/README.md](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/mmf_sa/models/README.md). Look for the models where `model_type: local`; these are the local models we import from [statsforecast](https://github.com/Nixtla/statsforecast) and [sktime](https://github.com/sktime/sktime). Check their documentations for the description of each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63971fae-0ab7-4bc6-a909-4a13eb8e02d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "active_models = [\n",
    "    \"StatsForecastBaselineWindowAverage\",\n",
    "    \"StatsForecastBaselineSeasonalWindowAverage\",\n",
    "    \"StatsForecastBaselineNaive\",\n",
    "    \"StatsForecastBaselineSeasonalNaive\",\n",
    "    \"StatsForecastAutoArima\",\n",
    "    \"StatsForecastAutoETS\",\n",
    "    \"StatsForecastAutoCES\",\n",
    "    \"StatsForecastAutoTheta\",\n",
    "    \"StatsForecastAutoTbats\",\n",
    "    \"StatsForecastAutoMfles\",\n",
    "    \"StatsForecastTSB\",\n",
    "    \"StatsForecastADIDA\",\n",
    "    \"StatsForecastIMAPA\",\n",
    "    \"StatsForecastCrostonClassic\",\n",
    "    \"StatsForecastCrostonOptimized\",\n",
    "    \"StatsForecastCrostonSBA\",\n",
    "    \"SKTimeProphet\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1551fc4-a443-44aa-9722-ca9f67619275",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run MMF\n",
    "\n",
    "Now, we can run the evaluation and forecasting using `run_forecast` function defined in [mmf_sa/models/__init__.py](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/mmf_sa/models/__init__.py). Make sure to set `freq=\"W\"` in `run_forecast` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd81648-5187-4229-a323-ef1ac61b9589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_forecast(\n",
    "    spark=spark,\n",
    "    train_data=f\"{catalog}.{db}.m4_weekly_train\",\n",
    "    scoring_data=f\"{catalog}.{db}.m4_weekly_train\",\n",
    "    scoring_output=f\"{catalog}.{db}.weekly_scoring_output\",\n",
    "    evaluation_output=f\"{catalog}.{db}.weekly_evaluation_output\",\n",
    "    group_id=\"unique_id\",\n",
    "    date_col=\"ds\",\n",
    "    target=\"y\",\n",
    "    freq=\"W\",\n",
    "    prediction_length=4,\n",
    "    backtest_length=12,\n",
    "    stride=1,\n",
    "    metric=\"smape\",\n",
    "    train_predict_ratio=1,\n",
    "    data_quality_check=True,\n",
    "    resample=False,\n",
    "    active_models=active_models,\n",
    "    experiment_path=f\"/Users/{user}/mmf/m4_weekly\",\n",
    "    use_case_name=\"m4_weekly\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3be29d7f-e4c8-4cd2-aced-1d20fb2d1712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluate\n",
    "In `evaluation_output` table, the we store all evaluation results for all backtesting trials from all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bdc5169-5525-45cb-a8f5-4d3074c9d91d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  spark.sql(f\"\"\"\n",
    "    select * from {catalog}.{db}.weekly_evaluation_output \n",
    "    where unique_id = 'W1'\n",
    "    order by unique_id, model, backtest_window_start_date\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c649314-421d-4fad-82c0-4c17fe0c3bbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Forecast\n",
    "In `scoring_output` table, forecasts for each time series from each model are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c546605-ce60-4b91-8299-c478090b19fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "                  select * from {catalog}.{db}.weekly_scoring_output \n",
    "                  where unique_id = 'W1'\n",
    "                  order by unique_id, model, ds\n",
    "                  \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "912c805e-60ef-41ab-8a08-0611bf8c0cfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Refer to the [notebook](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/examples/post-evaluation-analysis.ipynb) for guidance on performing fine-grained model selection after running `run_forecast`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c08a92d-936c-4e99-acec-62db9adc3b51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Delete Tables\n",
    "Let's clean up the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ee5b1fc-ab40-4dcf-9273-743531e33e3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(spark.sql(f\"delete from {catalog}.{db}.weekly_evaluation_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b0f366c-da81-49b3-afba-135718369bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(spark.sql(f\"delete from {catalog}.{db}.weekly_scoring_output\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "local_univariate_weekly",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
