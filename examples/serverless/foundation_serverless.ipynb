{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f1c7b37-b4cf-4a83-b1c7-40178dc2bc6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Many Models Forecasting Demo\n",
    "\n",
    "This notebook showcases how to run MMF on a serverless compute using foundation models. We will use [M4 competition](https://www.sciencedirect.com/science/article/pii/S0169207019301128#sec5) data. The descriptions here are mostly the same as the case with the [daily resolution](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/examples/daily/global_daily.ipynb), so we will skip the redundant parts and focus only on the essentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ed068c4-969c-4945-959e-dcaba797cc3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Serverless Compute setup\n",
    "\n",
    "Attach this notebook to a [serverless compute](https://docs.databricks.com/aws/en/compute/serverless/notebooks). Then go to the [Configuration](https://docs.databricks.com/aws/en/compute/serverless/dependencies) tab. Choose `A10` (GPU) in [Accelerator](https://docs.databricks.com/aws/en/compute/serverless/gpu) section and set the [Environment version](https://docs.databricks.com/aws/en/compute/serverless/dependencies#-select-an-environment-version) to 3. In the Dependencies section, [add the path](https://docs.databricks.com/aws/en/compute/serverless/dependencies#create-common-utilities-to-share-across-your-workspace) to your `many-model-forecasting` directory: e.g., `/Workspace/Users/ryuta.yoshimatsu@databricks.com/many-model-forecasting`. This is required to use the MMF functions within the notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60bd952b-6501-4c4b-ade4-4b9d4d0c7be6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74f7f7a9-0c1f-4c3b-a58b-cba052c896de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ../../requirements.txt --quiet\n",
    "%pip install --force-reinstall --no-cache-dir --index-url https://download.pytorch.org/whl/cu121 torch==2.3.1+cu121 torchvision==0.18.1+cu121 torchaudio==2.3.1+cu121 --quiet\n",
    "%pip install chronos-forecasting==1.4.1 --quiet\n",
    "%pip install uni2ts==1.2.0 --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efdcd23b-b350-4b5e-97ec-d4df18d15f97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"py4j.java_gateway\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j.clientserver\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3a8738f-1ed4-432f-937b-16b79d1257d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "from datasetsforecast.m4 import M4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eefd8d70-ae85-4c48-ae07-8526203db76c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare data \n",
    "We are using [`datasetsforecast`](https://github.com/Nixtla/datasetsforecast/tree/main/) package to download M4 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58c27e46-0dd9-4984-9a34-de0e59b02e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Number of time series\n",
    "n = 100\n",
    "\n",
    "\n",
    "def create_m4_monthly():\n",
    "    y_df, _, _ = M4.load(directory=str(pathlib.Path.home()), group=\"Monthly\")\n",
    "    _ids = [f\"M{i}\" for i in range(1, n + 1)]\n",
    "    y_df = (\n",
    "        y_df.groupby(\"unique_id\")\n",
    "        .filter(lambda x: x.unique_id.iloc[0] in _ids)\n",
    "        .groupby(\"unique_id\")\n",
    "        .apply(transform_group)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return y_df\n",
    "\n",
    "\n",
    "def transform_group(df):\n",
    "    unique_id = df.unique_id.iloc[0]\n",
    "    _cnt = 60  # df.count()[0]\n",
    "    _start = pd.Timestamp(\"2018-01-01\")\n",
    "    _end = _start + pd.DateOffset(months=_cnt)\n",
    "    date_idx = pd.date_range(start=_start, end=_end, freq=\"M\", name=\"date\")\n",
    "    _df = (\n",
    "        pd.DataFrame(data=[], index=date_idx)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"date\"})\n",
    "    )\n",
    "    _df[\"unique_id\"] = unique_id\n",
    "    _df[\"y\"] = df[:60].y.values\n",
    "    return _df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8ad4242-6b71-4184-9342-781eb6ba88b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We are going to save this data in a delta lake table. Provide catalog and database names where you want to store the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c1a4321-7fb1-459a-a853-f73a10807eb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"mmf\" # Name of the catalog we use to manage our assets\n",
    "db = \"m4\" # Name of the schema we use to manage our assets (e.g. datasets)\n",
    "user = spark.sql('select current_user() as user').collect()[0]['user'] # User email address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c50ab3a8-8dca-404e-aa82-baceb70ee309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Making sure that the catalog and the schema exist\n",
    "#_ = spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "#_ = spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{db}\")\n",
    "\n",
    "#(\n",
    "#    spark.createDataFrame(create_m4_monthly())\n",
    "#    .write.format(\"delta\").mode(\"overwrite\")\n",
    "#    .saveAsTable(f\"{catalog}.{db}.serverless_train\")\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "772e2e7a-c892-4dec-8a81-702747e57cf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's take a peak at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5126db5d-fbe0-4484-94c2-bbc2e07d112a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"select unique_id, count(date) as count from {catalog}.{db}.serverless_train group by unique_id order by unique_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94320b45-43a6-489d-aab4-19cb6c8cc5f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  spark.sql(f\"select * from {catalog}.{db}.serverless_train where unique_id in ('M1', 'M2', 'M3', 'M4', 'M5') order by unique_id, date\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99cace3a-1e9b-4e8c-ac32-402b923ba98c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note that monthly forecasting requires the timestamp column to represent the last day of each month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b72f5d4-a17b-4d04-9813-a3e75700c626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Models\n",
    "Let's configure a list of models we are going to apply to our time series for evaluation and forecasting. We install from [chronos](https://pypi.org/project/chronos-forecasting/) and [uni2ts](https://pypi.org/project/uni2ts/). Check their documentation for the detailed description of each model.\n",
    "\n",
    "**TimesFM is currently not supported for serverless compute in MMF.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "258d81bf-e031-4393-91f2-eaf81ab83eae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "active_models = [\n",
    "    #\"ChronosT5Tiny\",\n",
    "    #\"ChronosT5Mini\",\n",
    "    #\"ChronosT5Small\",\n",
    "    #\"ChronosT5Base\",\n",
    "    #\"ChronosT5Large\",\n",
    "    #\"ChronosBoltTiny\",\n",
    "    #\"ChronosBoltMini\",\n",
    "    #\"ChronosBoltSmall\",\n",
    "    #\"ChronosBoltBase\",\n",
    "    \"MoiraiSmall\",\n",
    "    #\"MoiraiBase\",\n",
    "    #\"MoiraiLarge\",\n",
    "    #\"MoiraiMoESmall\",\n",
    "    #\"MoiraiMoEBase\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9050c430-b6f7-4635-9a63-83a6050a1aef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run MMF\n",
    "\n",
    "Now, we can run the evaluation and forecasting using `run_forecast` function defined in [mmf_sa/models/__init__.py](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/mmf_sa/models/__init__.py). Refer to [README.md](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/README.md#parameters-description) for a comprehensive description of each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8ff2f60-a145-4ffd-a69c-aa4ecd178fae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cache_dir = f\"/Volumes/{catalog}/{db}/hf_cache\" \n",
    "os.makedirs(cache_dir, exist_ok=True) \n",
    "os.environ[\"HF_HOME\"] = cache_dir # low-level hub \n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = cache_dir # huggingface_hub >=0.20 \n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = cache_dir # transformers fallback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00e6a5ec-2b7e-4d50-b2a9-009e924c7db7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mmf_sa import run_forecast\n",
    "\n",
    "run_forecast(\n",
    "    spark=spark,\n",
    "    train_data=f\"{catalog}.{db}.serverless_train\",\n",
    "    scoring_data=f\"{catalog}.{db}.serverless_train\",\n",
    "    scoring_output=f\"{catalog}.{db}.serverless_scoring_output\",\n",
    "    evaluation_output=f\"{catalog}.{db}.serverless_evaluation_output\",\n",
    "    model_output=f\"{catalog}.{db}\",\n",
    "    group_id=\"unique_id\",\n",
    "    date_col=\"date\",\n",
    "    target=\"y\",\n",
    "    freq=\"M\",\n",
    "    prediction_length=3,\n",
    "    backtest_length=12,\n",
    "    stride=1,\n",
    "    metric=\"smape\",\n",
    "    train_predict_ratio=1,\n",
    "    data_quality_check=True,\n",
    "    resample=False,\n",
    "    active_models=active_models,\n",
    "    experiment_path=f\"/Users/{user}/mmf/serverless\",\n",
    "    use_case_name=\"serverless\",\n",
    "    accelerator=\"gpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b416295b-da31-460c-b9e3-510d84f98d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluate\n",
    "In `evaluation_output` table, the we store all evaluation results for all backtesting trials from all models. This information can be used to understand which models performed well on which time series on which periods of backtesting. This is very important for selecting the final model for forecasting or models for ensembling. Maybe, it's faster to take a look at the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c36043ee-b91e-4991-98ae-d211b39f677e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "    select * from {catalog}.{db}.serverless_evaluation_output \n",
    "    where unique_id = 'M1'\n",
    "    order by unique_id, model, backtest_window_start_date\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5208a646-f74a-4e59-bfe5-f214fb5eed45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Forecast\n",
    "In `scoring_output` table, forecasts for each time series from each model are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5bac908-8275-4ba2-b2a5-9079f3615a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "    select * from {catalog}.{db}.serverless_scoring_output \n",
    "    where unique_id = 'M1'\n",
    "    order by unique_id, model, date\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eed9f43b-b2e6-4570-bacd-2101e5098c88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Refer to the [notebook](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/examples/post-evaluation-analysis.ipynb) for guidance on performing fine-grained model selection after running `run_forecast`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aa5efc1-50c7-472f-b0ff-e8a9625cf8f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Delete Tables\n",
    "Let's clean up the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c4a40e-4dda-4fb5-a969-19a400b19c16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(spark.sql(f\"delete from {catalog}.{db}.serverless_evaluation_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cef4f309-6a0a-4f24-9bf4-eab79e08dd6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(spark.sql(f\"delete from {catalog}.{db}.serverless_scoring_output\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "/Workspace/Users/ryuta.yoshimatsu@databricks.com/many-model-forecasting"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "foundation_serverless",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
