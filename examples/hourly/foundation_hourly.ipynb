{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fea43b0-4b85-4871-950d-15b137b20cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Many Models Forecasting Demo\n",
    "This notebook showcases how to run MMF with foundation models on multiple time series of hourly resolution. We will use [M4 competition](https://www.sciencedirect.com/science/article/pii/S0169207019301128#sec5) data. The descriptions here are mostly the same as the case with the [daily resolution](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/examples/daily/foundation_daily.py), so we will skip the redundant parts and focus only on the essentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05b8d07e-284f-4c5d-b9dc-7604e7d74606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cluster setup\n",
    "\n",
    "We recommend using a cluster with [Databricks Runtime 15.4 LTS for ML](https://docs.databricks.com/en/release-notes/runtime/15.4lts-ml.html). The cluster should be single-node with one or more GPU instances: e.g. [g4dn.12xlarge [T4]](https://aws.amazon.com/ec2/instance-types/g4/) on AWS or [Standard_NC64as_T4_v3](https://learn.microsoft.com/en-us/azure/virtual-machines/nct4-v3-series) on Azure. MMF leverages [Pandas UDF](https://docs.databricks.com/en/udf/pandas.html) for distributing the inference tasks and utilizing all the available resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42c2e04e-1751-45fc-a6d0-5bf8b76c70af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install and import packages\n",
    "Check out [requirements-foundation.txt](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/requirements-foundation.txt) if you're interested in the libraries we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c0bea0f-d8b0-404e-8aea-6d8852204de3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install datasetsforecast==0.0.8 --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0af57f1-aeea-42b6-bb0f-b66dc8f7504b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = spark._jvm.org.apache.log4j\n",
    "logging.getLogger(\"py4j.java_gateway\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j.clientserver\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7b16056-3d59-4b8f-aa06-c75d41e641dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from datasetsforecast.m4 import M4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d718ea6-5c00-4d1d-9f8a-c3f79e3da925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare data \n",
    "We are using [`datasetsforecast`](https://github.com/Nixtla/datasetsforecast/tree/main/) package to download M4 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fad1fde-bfc2-43d6-abce-d9c3f4611892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Number of time series\n",
    "n = 100\n",
    "\n",
    "\n",
    "def create_m4_hourly():\n",
    "    y_df, _, _ = M4.load(directory=str(pathlib.Path.home()), group=\"Hourly\")\n",
    "    _ids = [f\"H{i}\" for i in range(1, n)]\n",
    "    y_df = (\n",
    "        y_df.groupby(\"unique_id\")\n",
    "        .filter(lambda x: x.unique_id.iloc[0] in _ids)\n",
    "        .groupby(\"unique_id\")\n",
    "        .apply(transform_group)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return y_df\n",
    "\n",
    "\n",
    "def transform_group(df):\n",
    "    unique_id = df.unique_id.iloc[0]\n",
    "    if len(df) > 720:\n",
    "        df = df.iloc[-720:]\n",
    "    _start = pd.Timestamp(\"2025-01-01 00:00\")\n",
    "    _end = _start + pd.DateOffset(hours=len(df)-1)\n",
    "    date_idx = pd.date_range(start=_start, end=_end, freq=\"H\", name=\"ds\")\n",
    "    res_df = pd.DataFrame(data=[], index=date_idx).reset_index()\n",
    "    res_df[\"unique_id\"] = unique_id\n",
    "    res_df[\"y\"] = df.y.values\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "218553ff-0536-48b9-be94-f4e89ce44dee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We are going to save this data in a delta lake table. Provide catalog and database names where you want to store the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "281e31fe-acc0-430c-9f5c-cfc6776ba04e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"mmf\" # Name of the catalog we use to manage our assets\n",
    "db = \"m4\" # Name of the schema we use to manage our assets (e.g. datasets)\n",
    "user = spark.sql('select current_user() as user').collect()[0]['user'] # User email address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc1d292a-fa55-4299-8bcf-c99cd4224214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Making sure that the catalog and the schema exist\n",
    "_ = spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "_ = spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{db}\")\n",
    "\n",
    "(\n",
    "    spark.createDataFrame(create_m4_hourly())\n",
    "    .write.format(\"delta\").mode(\"overwrite\")\n",
    "    .saveAsTable(f\"{catalog}.{db}.m4_hourly_train\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58921d70-5b89-4a76-8a67-0e7925e62f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's take a peak at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "791c147c-efb4-4419-8bf0-24a0ff72186b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"select unique_id, count(ds) as count from {catalog}.{db}.m4_hourly_train group by unique_id order by unique_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b297ca42-d66f-4034-b788-5cec21a5e029",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  spark.sql(f\"select * from {catalog}.{db}.m4_hourly_train where unique_id in ('H1', 'H2', 'H3', 'H4', 'H5') order by unique_id, ds\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12d14bd8-d54f-46ab-a854-01787a42a646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Models\n",
    "Let's configure a list of models we are going to apply to our time series for evaluation and forecasting. A comprehensive list of all supported models is available in [mmf_sa/models/README.md](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/mmf_sa/models/README.md). Look for the models where `model_type: foundation`; these are the foundation models we install from [chronos](https://pypi.org/project/chronos-forecasting/), [uni2ts](https://pypi.org/project/uni2ts/) and [timesfm](https://pypi.org/project/timesfm/). Check their documentation for the detailed description of each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37405d59-25e7-4592-97d7-490016e8b42f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "active_models = [\n",
    "    \"ChronosT5Tiny\",\n",
    "    \"ChronosT5Mini\",\n",
    "    \"ChronosT5Small\",\n",
    "    \"ChronosT5Base\",\n",
    "    \"ChronosT5Large\",\n",
    "    \"ChronosBoltTiny\",\n",
    "    \"ChronosBoltMini\",\n",
    "    \"ChronosBoltSmall\",\n",
    "    \"ChronosBoltBase\",\n",
    "    \"MoiraiSmall\",\n",
    "    \"MoiraiBase\",\n",
    "    \"MoiraiLarge\",\n",
    "    \"MoiraiMoESmall\",\n",
    "    \"MoiraiMoEBase\",\n",
    "    \"TimesFM_1_0_200m\",\n",
    "    \"TimesFM_2_0_500m\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46bcbec8-0fe6-4fab-9a3f-0a8ccac041da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run MMF\n",
    "\n",
    "Now, we can run the evaluation and forecasting using `run_forecast` function defined in [mmf_sa/models/__init__.py](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/mmf_sa/models/__init__.py). Refer to [README.md](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/README.md#parameters-description) for a comprehensive description of each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c01cd3-3b05-4755-8b54-dbafe33f33ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The same run_id will be assigned to all the models. This makes it easier to run the post evaluation analysis later.\n",
    "run_id = str(uuid.uuid4())\n",
    "\n",
    "for model in active_models:\n",
    "  dbutils.notebook.run(\n",
    "    \"../run_hourly\",\n",
    "    timeout_seconds=0,\n",
    "    arguments={\"catalog\": catalog, \"db\": db, \"model\": model, \"run_id\": run_id, \"user\": user})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5f1ded2-cda4-4eca-afc0-03e8715374ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluate\n",
    "In `evaluation_output` table, the we store all evaluation results for all backtesting trials from all models. This information can be used to understand which models performed well on which time series on which periods of backtesting. This is very important for selecting the final model for forecasting or models for ensembling. Maybe, it's faster to take a look at the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfaeadab-4e9e-4165-959e-02aa990745df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "    select * from {catalog}.{db}.hourly_evaluation_output \n",
    "    where unique_id = 'H1'\n",
    "    order by unique_id, model, backtest_window_start_date\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df88bdd8-c44d-4ecc-9677-64655f2c86cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Forecast\n",
    "In `scoring_output` table, forecasts for each time series from each model are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "917156b3-699f-421f-871b-df7ab4c9d0a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "    select * from {catalog}.{db}.hourly_scoring_output \n",
    "    where unique_id = 'H1'\n",
    "    order by unique_id, model, ds\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a7ed8a-1c8d-461f-b0e0-a8a6e7aa09ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Refer to the [notebook](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/examples/post-evaluation-analysis.ipynb) for guidance on performing fine-grained model selection after running `run_forecast`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb8ba3d1-d4ed-4bb4-8925-719337d267e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Delete Tables\n",
    "Let's clean up the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e3267f3-8fa8-4da3-b435-5216f3e6a7f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(spark.sql(f\"delete from {catalog}.{db}.hourly_evaluation_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67bf5704-5efc-4cb2-b6db-0d0b8151aaf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(spark.sql(f\"delete from {catalog}.{db}.hourly_scoring_output\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "foundation_hourly",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
