{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "8b9407da-ea51-4963-ba06-4510218d9c87",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "# Many Models Forecasting Demo\n",
        "This notebook showcases how to run MMF with global models on multiple time series of hourly resolution. We will use [M4 competition](https://www.sciencedirect.com/science/article/pii/S0169207019301128#sec5) data. The descriptions here are mostly the same as the case with the [daily resolution](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/examples/daily/global_daily.ipynb), so we will skip the redundant parts and focus only on the essentials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "851fabbf-f284-499f-a664-15bc3910f286",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "### Cluster setup\n",
        "\n",
        "We recommend using a cluster with [Databricks Runtime 18.1 for ML](https://docs.databricks.com/en/release-notes/runtime/18.1-ml.html) or later. Use a single-node or multi-node GPU cluster with A10G instances: e.g. [g5.12xlarge [A10G]](https://aws.amazon.com/ec2/instance-types/g5/) on AWS or [Standard_NV36ads_A10_v5](https://learn.microsoft.com/en-us/azure/virtual-machines/nva10v5-series) on Azure. A single-node multi-GPU cluster is generally recommended. For multi-node clusters, set `num_nodes` below to the number of worker nodes and disable autoscaling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "51ffa3ec-0111-4071-8298-473baee32e24",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "### Install and import packages\n",
        "Check out [requirements-global.txt](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/requirements-global.txt) if you're interested in the libraries we use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9a2f059c-44ee-4beb-94fe-9d5b37aeb217",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "%pip install datasetsforecast==0.0.8 --quiet\n",
        "dbutils.library.restartPython()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "703dc989-cea3-4e5f-8e51-691891113824",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import uuid\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "from datasetsforecast.m4 import M4\n",
        "\n",
        "logging.getLogger(\"py4j.clientserver\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"py4j.java_gateway\").setLevel(logging.WARNING)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7d8c3502-10fe-465a-9784-9b5cd20d1392",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "### Prepare data \n",
        "We are using [`datasetsforecast`](https://github.com/Nixtla/datasetsforecast/tree/main/) package to download M4 data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "38cd85be-5614-41db-b619-1ec76a1c9044",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Number of time series\n",
        "n = 100\n",
        "\n",
        "\n",
        "def create_m4_hourly():\n",
        "    y_df, _, _ = M4.load(directory=str(pathlib.Path.home()), group=\"Hourly\")\n",
        "    target_ids = {f\"H{i}\" for i in range(1, n)}\n",
        "    y_df = y_df[y_df[\"unique_id\"].isin(target_ids)]\n",
        "    y_df = (\n",
        "        y_df.groupby(\"unique_id\", group_keys=False)\n",
        "             .apply(lambda g: transform_group(g, g.name))\n",
        "             .reset_index(drop=True)\n",
        "    )\n",
        "    return y_df\n",
        "\n",
        "\n",
        "def transform_group(df, unique_id):\n",
        "    if len(df) > 720:\n",
        "        df = df.iloc[-720:]\n",
        "    start = pd.Timestamp(\"2025-01-01 00:00\")\n",
        "    date_idx = pd.date_range(start=start, periods=len(df), freq=\"h\", name=\"ds\")\n",
        "    res_df = pd.DataFrame({\n",
        "        \"ds\": date_idx,\n",
        "        \"unique_id\": unique_id,\n",
        "        \"y\": df[\"y\"].to_numpy()\n",
        "    })\n",
        "    return res_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b85b48d5-8d41-4fbc-810a-162704cf0329",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "We are going to save this data in a delta lake table. Provide catalog and database names where you want to store the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1c8dcbdf-35f8-415f-a047-1eee555f727d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "catalog = \"mmf\" # Name of the catalog we use to manage our assets\n",
        "db = \"m4\" # Name of the schema we use to manage our assets (e.g. datasets)\n",
        "user = spark.sql('select current_user() as user').collect()[0]['user'] # User email address"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "58f89f5a-3c09-463f-8c23-68df42ebac23",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Making sure that the catalog and the schema exist\n",
        "_ = spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
        "_ = spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{db}\")\n",
        "\n",
        "(\n",
        "    spark.createDataFrame(create_m4_hourly())\n",
        "    .write.format(\"delta\").mode(\"overwrite\")\n",
        "    .saveAsTable(f\"{catalog}.{db}.m4_hourly_train\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "47b9ba90-0d67-409e-a4d1-ca39954c86f2",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "Let's take a peak at the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "70259e1b-87de-4f7e-b9c6-e2222abfd83d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "display(spark.sql(f\"select unique_id, count(ds) as count from {catalog}.{db}.m4_hourly_train group by unique_id order by unique_id\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "422c850c-b47b-4fa0-8f6c-ce78d28b0785",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "display(\n",
        "  spark.sql(f\"select * from {catalog}.{db}.m4_hourly_train where unique_id in ('H1', 'H2', 'H3', 'H4', 'H5') order by unique_id, ds\")\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e14fad28-3fb3-4d6b-8439-fc202a8280dd",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "### Models\n",
        "Let's configure a list of models we are going to apply to our time series for evaluation and forecasting. A comprehensive list of all supported models is available in [mmf_sa/models/README.md](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/mmf_sa/models/README.md). Look for the models where `model_type: global`; these are the global models we import from [neuralforecast](https://github.com/Nixtla/neuralforecast). Check their documentation for the detailed description of each model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "661a93e5-e869-41fb-b47c-2f421949f635",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "active_models = [\n",
        "    \"NeuralForecastRNN\",\n",
        "    \"NeuralForecastLSTM\",\n",
        "    \"NeuralForecastNBEATSx\",\n",
        "    \"NeuralForecastNHITS\",\n",
        "    \"NeuralForecastAutoRNN\",\n",
        "    \"NeuralForecastAutoLSTM\",\n",
        "    \"NeuralForecastAutoNBEATSx\",\n",
        "    \"NeuralForecastAutoNHITS\",\n",
        "    \"NeuralForecastAutoTiDE\",\n",
        "    \"NeuralForecastAutoPatchTST\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "27f30e6f-42a8-4784-bf81-5c44c2c80ed8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "### Run MMF\n",
        "\n",
        "Now, we can run the evaluation and forecasting using `run_forecast` function defined in [mmf_sa/models/__init__.py](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/mmf_sa/models/__init__.py). Refer to [README.md](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/README.md#parameters-description) for a comprehensive description of each parameter. Make sure to set `freq=\"H\"` in `run_forecast` function called in [examples/run_hourly.ipynb](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/examples/run_hourly.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f4cf9d05-1c49-4392-9777-6a4a258f2e32",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Number of nodes for distributed training. Use 1 for single-node multi-GPU,\n",
        "# or set to the number of worker nodes for multi-node multi-GPU clusters.\n",
        "num_nodes = 1\n",
        "\n",
        "# The same run_id will be assigned to all the models. This makes it easier to run the post evaluation analysis later.\n",
        "run_id = str(uuid.uuid4())\n",
        "\n",
        "for model in active_models:\n",
        "  dbutils.notebook.run(\n",
        "    \"../run_hourly\",\n",
        "    timeout_seconds=0,\n",
        "    arguments={\"catalog\": catalog, \"db\": db, \"model\": model, \"run_id\": run_id, \"user\": user, \"num_nodes\": str(num_nodes)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "988a7636-3144-4fec-8974-b7fbb3834cdf",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "### Evaluate\n",
        "In `evaluation_output` table, the we store all evaluation results for all backtesting trials from all models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "143b10cb-8bf8-4052-b036-80d379653a87",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "display(spark.sql(f\"\"\"\n",
        "    select * from {catalog}.{db}.hourly_evaluation_output \n",
        "    where unique_id = 'H1'\n",
        "    order by unique_id, model, backtest_window_start_date\n",
        "    \"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "98780472-1d42-4507-bcb6-4733d1a3c09e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "### Forecast\n",
        "In `scoring_output` table, forecasts for each time series from each model are stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1e9312d5-01be-4d9f-b631-2ed607c3cdbe",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "display(spark.sql(f\"\"\"\n",
        "    select * from {catalog}.{db}.hourly_scoring_output \n",
        "    where unique_id = 'H1'\n",
        "    order by unique_id, model, ds\n",
        "    \"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "857b33a2-a69a-4e06-b952-46efb0ff12db",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "Refer to the [notebook](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/examples/post-evaluation-analysis.ipynb) for guidance on performing fine-grained model selection after running `run_forecast`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7dbb6e50-955c-48cf-ab73-e45e08bc32b9",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "### Delete Tables\n",
        "Let's clean up the tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6fc0892a-7798-4b2d-8546-e7d788de6ffb",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "#display(spark.sql(f\"delete from {catalog}.{db}.hourly_evaluation_output\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "338b9d42-deae-44a6-8450-37a9b3dcde3a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "#display(spark.sql(f\"delete from {catalog}.{db}.hourly_scoring_output\"))"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": {
        "hardware": {
          "accelerator": null,
          "gpuPoolId": null,
          "memory": null
        }
      },
      "dashboards": [],
      "environmentMetadata": null,
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "global_hourly",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
