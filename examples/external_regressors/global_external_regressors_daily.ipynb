{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4a7a13c5-cf03-4794-ad25-c87c70017bb0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "# Many Models Forecasting Demo\n",
        "\n",
        "This notebook showcases how to run MMF with global models on multiple time series of daily resolution using exogenous regressors. We will use [Rossmann Store](https://www.kaggle.com/competitions/rossmann-store-sales/data) data. To be able to run this notebook, you need to register on [Kaggle](https://www.kaggle.com/) and download the dataset. The descriptions here are mostly the same as the case [without exogenous regressors](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/examples/daily/global_daily.ipynb), so we will skip the redundant parts and focus only on the essentials. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c9e60d52-5ad3-44c1-b3d8-6e09353d7eac",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "### Cluster setup\n",
        "\n",
        "We recommend using a cluster with [Databricks Runtime 18.1 for ML](https://docs.databricks.com/en/release-notes/runtime/18.1-ml.html) or later. Use a single-node or multi-node GPU cluster with A10G instances: e.g. [g5.12xlarge [A10G]](https://aws.amazon.com/ec2/instance-types/g5/) on AWS or [Standard_NV36ads_A10_v5](https://learn.microsoft.com/en-us/azure/virtual-machines/nva10v5-series) on Azure. A single-node multi-GPU cluster is generally recommended. For multi-node clusters, set `num_nodes` below to the number of worker nodes and disable autoscaling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "14ebd8ef-fc5a-4577-927d-75a86e7bb84d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "### Install and import packages\n",
        "Check out [requirements-global.txt](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/requirements-global.txt) if you're interested in the libraries we use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c70c986b-b70d-4ae0-8777-0af941551d8d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "%pip install datasetsforecast==0.0.8 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "18d4c427-8c64-4ec0-a250-0dbe02bbe000",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from tqdm.autonotebook import tqdm\n",
        "import uuid\n",
        "\n",
        "logging.getLogger(\"py4j.clientserver\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"py4j.java_gateway\").setLevel(logging.WARNING)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f14eeaea-7efb-4854-a09d-19dedf62f507",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "### Prepare data \n",
        "Before running this notebook, download the dataset from [Kaggle](https://www.kaggle.com/competitions/rossmann-store-sales/data) and store them in Unity Catalog as a [volume](https://docs.databricks.com/en/connect/unity-catalog/volumes.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "acd7f3b0-1213-458f-bda5-5deffe85568f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "catalog = \"mmf\" # Name of the catalog we use to manage our assets\n",
        "db = \"rossmann\" # Name of the schema we use to manage our assets (e.g. datasets)\n",
        "volume = \"csv\" # Name of the volume where you have your rossmann dataset csv sotred\n",
        "user = spark.sql('select current_user() as user').collect()[0]['user'] # User email address"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a27907b6-96f1-4b21-a077-f30adc1d555f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Make sure that the catalog and the schema exist\n",
        "_ = spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
        "_ = spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{db}\")\n",
        "_ = spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{db}.{volume}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b5c3f7fa-cede-4d93-9b48-c9f88f11740a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Randomly select 100 stores to forecast\n",
        "import random\n",
        "random.seed(7)\n",
        "\n",
        "# Number of time series to sample\n",
        "sample = True\n",
        "size = 100\n",
        "stores = sorted(random.sample(range(0, 1000), size))\n",
        "\n",
        "train = spark.read.csv(f\"/Volumes/{catalog}/{db}/{volume}/train.csv\", header=True, inferSchema=True)\n",
        "test = spark.read.csv(f\"/Volumes/{catalog}/{db}/{volume}/test.csv\", header=True, inferSchema=True)\n",
        "\n",
        "if sample:\n",
        "    train = train.filter(train.Store.isin(stores))\n",
        "    test = test.filter(test.Store.isin(stores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9fb4afac-9a6e-4c4b-93c1-4f5a8086309e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "We are going to save this data in a delta lake table. Provide catalog and database names where you want to store the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ceea92ae-595d-4345-83a4-e2eaf96a71f3",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "train.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog}.{db}.rossmann_daily_train\")\n",
        "test.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog}.{db}.rossmann_daily_test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3353e346-3c25-4b4e-9488-bb36be15f951",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "Let's take a peak at the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0883554d-d86f-4ade-86b4-109a45183d10",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "display(spark.sql(f\"select * from {catalog}.{db}.rossmann_daily_train where Store=49 order by Date\"))\n",
        "display(spark.sql(f\"select * from {catalog}.{db}.rossmann_daily_test where Store=49 order by Date\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d4c31c09-8a69-4b5b-8089-be8b2d2312a6",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "Note that in `rossmann_daily_train` we have our target variable `Sales` but not in `rossmann_daily_test`. This is because `rossmann_daily_test` is going to be used as our `scoring_data` that stores `dynamic_future_categorical` variables of the future dates. When you adapt this notebook to your use case, make sure to comply with these datasets formats. See neuralforecast's [documentation](https://nixtlaverse.nixtla.io/neuralforecast/examples/exogenous_variables.html) for more detail on exogenous regressors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2a049a6b-f189-4204-8eb2-bf293f6bbadf",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "### Models\n",
        "Let's configure a list of models we are going to apply to our time series for evaluation and forecasting. A comprehensive list of all supported models is available in [mmf_sa/models/README.md](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/mmf_sa/models/README.md). Look for the models where `model_type: global`; these are the global models we import from [neuralforecast](https://github.com/Nixtla/neuralforecast). Check their documentation for the detailed description of each model. \n",
        "\n",
        "Exogenous regressors are currently only supported for [some models](https://nixtlaverse.nixtla.io/neuralforecast/models.html) (e.g. `NeuralForecastAutoNBEATSx`). The following list of models support exogenous regressors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0a0276fe-c533-4edf-822f-43d50f713fad",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "active_models = [\n",
        "    \"NeuralForecastRNN\",\n",
        "    \"NeuralForecastLSTM\",\n",
        "    \"NeuralForecastNBEATSx\",\n",
        "    \"NeuralForecastNHITS\",\n",
        "    \"NeuralForecastAutoRNN\",\n",
        "    \"NeuralForecastAutoLSTM\",\n",
        "    \"NeuralForecastAutoNBEATSx\",\n",
        "    \"NeuralForecastAutoNHITS\",\n",
        "    \"NeuralForecastAutoTiDE\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "da2d5ce0-e225-4321-9a6b-106963d96f22",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "### Run MMF\n",
        "\n",
        "Now, we run the evaluation and forecasting using `run_forecast` function. We are providing the training table and the scoring table names. If `scoring_data` is not provided or if the same name as `train_data` is provided, the models will ignore the `dynamic_future_numerical` and `dynamic_future_categorical` regressors. Note that we are providing a covariate field (i.e. `dynamic_future_numerical` or `dynamic_future_categorical`) this time in `run_forecast` function called in [examples/run_external_regressors_daily.ipynb](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/examples/run_external_regressors_daily.ipynb). There are also other convariate fields, namely `static_features`, and `dynamic_historical_numerical` and `dynamic_historical_categorical`, which you can provide. Read more about these covariates in [neuralforecast's documentation](https://nixtlaverse.nixtla.io/neuralforecast/examples/exogenous_variables.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "17ff778b-f140-43c0-a5bb-189c225f861b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Number of nodes for distributed training. Use 1 for single-node multi-GPU,\n",
        "# or set to the number of worker nodes for multi-node multi-GPU clusters.\n",
        "num_nodes = 1\n",
        "\n",
        "# The same run_id will be assigned to all the models. This makes it easier to run the post evaluation analysis later.\n",
        "run_id = str(uuid.uuid4())\n",
        "\n",
        "for model in active_models:\n",
        "  dbutils.notebook.run(\n",
        "    \"../run_external_regressors_daily\",\n",
        "    timeout_seconds=0,\n",
        "    arguments={\"catalog\": catalog, \"db\": db, \"model\": model, \"run_id\": run_id, \"user\": user, \"num_nodes\": str(num_nodes)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e75f9fd2-3b9e-4e2a-8cf8-0c8dea2bfbc2",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "### Evaluate\n",
        "In `evaluation_output` table, the we store all evaluation results for all backtesting trials from all models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "50c76b7b-90ae-47f5-bd77-1ae491a9284b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "display(\n",
        "  spark.sql(f\"\"\"\n",
        "            select * from {catalog}.{db}.rossmann_daily_evaluation_output \n",
        "            where Store=49\n",
        "            order by Store, model, backtest_window_start_date\n",
        "            \"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1cd4bf17-a9e0-4e53-ba78-691164bca057",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "### Forecast\n",
        "In `scoring_output` table, forecasts for each time series from each model are stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "294f3fef-6ed5-46b4-a610-2de757d853dd",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "display(spark.sql(f\"\"\"\n",
        "                  select * from {catalog}.{db}.rossmann_daily_scoring_output \n",
        "                  where Store=49 \n",
        "                  order by Store, model\n",
        "                  \"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "23e46439-d03c-49cf-9531-cadf6671bae7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "Refer to the [notebook](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/examples/post-evaluation-analysis.ipynb) for guidance on performing fine-grained model selection after running `run_forecast`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0ac04df0-181f-4ed0-904b-2047f9e2c421",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "### Delete Tables\n",
        "Let's clean up the tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "52251643-fe6e-4912-90ea-98cc277a80b7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "#display(spark.sql(f\"delete from {catalog}.{db}.rossmann_daily_evaluation_output\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "88b562bb-bee0-491d-ac08-a2ac69aabdda",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "#display(spark.sql(f\"delete from {catalog}.{db}.rossmann_daily_scoring_output\"))"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": null,
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "global_external_regressors_daily",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
