{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e2ce813-1aee-44e9-9517-f98e63f23a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Many Models Forecasting Demo\n",
    "\n",
    "This notebook demonstrates how to conduct fine-grained model selection after running the `mmf.run_forecast` function. Before proceeding, ensure you have run the notebooks in [`/examples/monthly`](https://github.com/databricks-industry-solutions/many-model-forecasting/tree/main/examples/monthly). You can run this notebook on a serverless compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bf99e2b-dcf8-44ca-a48b-37e162f1767f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"mmf\"  # Name of the catalog we use to manage our assets\n",
    "db = \"m4\"             # Name of the schema we use to manage our assets (e.g. datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f004c3a-03ee-4a1a-9587-7e69e9cf505c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the `scoring_output` table, forecasts for each time series from every model are stored. Let's filter by a specific time series (e.g., `M1`) and examine the forecasts from all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa631669-768f-45b7-a149-398094efec3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scoring_output =  spark.sql(f\"\"\"\n",
    "    SELECT model, unique_id, date, y FROM {catalog}.{db}.monthly_scoring_output \n",
    "    WHERE unique_id='M1' ORDER BY model\n",
    "    \"\"\")\n",
    "\n",
    "display(scoring_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "768b3d5c-f351-43b1-b49a-6356df69a2dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This table contains forecasts from 44 different models, but we need to determine which one is best for making business decisions. This is where the `evaluation_output` table becomes useful. Let's filter by a specific time series (e.g., `M1`) and review the evaluation results (i.e., backtesting trials) from all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb2c82f8-7690-47a5-98c2-ee3368dcc027",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"actual\":380,\"model\":130},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752073722386}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluation_output =  spark.sql(f\"\"\"\n",
    "    SELECT model, unique_id, backtest_window_start_date, metric_name, metric_value, forecast, actual \n",
    "    FROM {catalog}.{db}.monthly_evaluation_output where unique_id='M1'\n",
    "    order by model, backtest_window_start_date\n",
    "    \"\"\")\n",
    "\n",
    "display(evaluation_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "824a3595-1493-4e60-981d-5945f2dc08cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Based on the backtesting configuration, defined by the parameters `backtest_length`, `prediction_length`, and `stride` in the `mmf_sa.run_forecast` function, we obtain results from 10 backtesting trials for each model. For each trial, both forecasts and actual values are stored, enabling you to compute evaluation metrics based on residuals. Additionally, this table includes a built-in metric for quick assessment, which can be specified using the `metric` parameter. In this case, the metric is `smape`, and currently, `mae`, `mse`, `rmse`, `mape`, and `smape` are supported.\n",
    "\n",
    "We compute the mean `smape` across 10 backtesting trials for each model and each time series. The model with the lowest mean `smape` is then selected for each time series, and its forecast is retrieved from the `forecast_output` table. Below is a SQL query that performs this selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e89219-3725-430f-8be3-de4817177f43",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752064859501}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast_best_model = spark.sql(f\"\"\"\n",
    "    SELECT eval.unique_id, eval.model, eval.average_smape, score.date, score.y\n",
    "    FROM \n",
    "    (\n",
    "      SELECT unique_id, model, average_smape,\n",
    "      RANK() OVER (PARTITION BY unique_id ORDER BY average_smape ASC) AS rank\n",
    "      FROM (\n",
    "        SELECT unique_id, model, AVG(metric_value) AS average_smape\n",
    "        FROM {catalog}.{db}.monthly_evaluation_output\n",
    "        GROUP BY unique_id, model) \n",
    "        ORDER BY unique_id, rank\n",
    "    ) AS eval\n",
    "    INNER JOIN {catalog}.{db}.monthly_scoring_output AS score \n",
    "      ON eval.unique_id=score.unique_id AND eval.model=score.model\n",
    "    WHERE eval.rank=1\n",
    "    ORDER BY eval.unique_id\n",
    "    \"\"\")\n",
    "\n",
    "display(forecast_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdc7d8c6-0410-43f7-8f85-6901d986ebf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "These forecasts will be used to guide our business decisions. Let's count how many times each model was the best-performing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f94dc20-e26f-4906-9f9c-635c73181038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_ranking = spark.sql(f\"\"\"\n",
    "    SELECT model, count(*) as count\n",
    "    FROM (\n",
    "      SELECT unique_id, model, average_smape,\n",
    "      RANK() OVER (PARTITION BY unique_id ORDER BY average_smape ASC) AS rank\n",
    "      FROM (\n",
    "        SELECT unique_id, model, AVG(metric_value) AS average_smape\n",
    "        FROM {catalog}.{db}.monthly_evaluation_output\n",
    "        GROUP BY unique_id, model) \n",
    "        ORDER BY unique_id, rank\n",
    "    ) WHERE rank=1 GROUP BY model \n",
    "    ORDER BY count DESC\n",
    "    \"\"\")\n",
    "\n",
    "display(model_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183eb27a-a19a-4967-aa65-26b6f5e8ce11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "On this dataset (M4 monthly), it appears that TimesFM models were the best performing models based on the number of time series they excelled in.\n",
    "\n",
    "Exposing the `evaluation_output` and `forecast_output` tables in these formats provides great flexibility in model selection. For example, you can define your own evaluation metric to compare forecasting accuracy. You can aggregate metrics using a weighted average or even the median across backtesting trials. Additionally, you can retrieve forecasts from multiple models for each time series and ensemble them. All of these options simply require writing queries against these tables."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "post-evaluation-analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
