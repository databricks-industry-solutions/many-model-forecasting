{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90f13a7a-1c10-4ce5-98b7-1362bef08152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Many Models Forecasting Demo\n",
    "\n",
    "This notebook showcases how to run MMF with local models on multiple univariate time series of monthly resolution. We will use [M4 competition](https://www.sciencedirect.com/science/article/pii/S0169207019301128#sec5) data. The descriptions here are mostly the same as the case with the [daily resolution](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/examples/daily/local_univariate_daily.ipynb), so we will skip the redundant parts and focus only on the essentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c86bfc3-40f2-4b44-8ba9-114d6325278b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cluster setup\n",
    "\n",
    "We recommend using a cluster with [Databricks Runtime 16.4 LTS for ML](https://docs.databricks.com/en/release-notes/runtime/16.4lts-ml.html). The cluster can be either a single-node or multi-node CPU cluster. Make sure to set the following Spark configurations before you start your cluster: [`spark.sql.execution.arrow.enabled true`](https://spark.apache.org/docs/3.0.1/sql-pyspark-pandas-with-arrow.html#enabling-for-conversion-tofrom-pandas) and [`spark.sql.adaptive.enabled false`](https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution). You can do this by specifying [Spark configuration](https://docs.databricks.com/en/compute/configure.html#spark-configuration) in the advanced options on the cluster creation page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f3ba55e-3458-4a03-80d1-2e9039a4a52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f990272-6f87-4bd4-9d92-23b772d58934",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install the necessary libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ../../requirements.txt --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34973999-253b-419b-9ed6-185eb494703a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from tqdm.autonotebook import tqdm\n",
    "logging.getLogger(\"py4j.java_gateway\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j.clientserver\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1d691d7-2798-4a78-8a9b-c4de82f55da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "from datasetsforecast.m4 import M4\n",
    "from mmf_sa import run_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e86743e-7317-458c-bbc0-9d27c86e4eb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare data \n",
    "We are using [`datasetsforecast`](https://github.com/Nixtla/datasetsforecast/tree/main/) package to download M4 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a598cdc1-0053-4528-9e4d-92e51c2f547c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Number of time series\n",
    "n = 100\n",
    "\n",
    "\n",
    "def create_m4_monthly():\n",
    "    y_df, _, _ = M4.load(directory=str(pathlib.Path.home()), group=\"Monthly\")\n",
    "    _ids = [f\"M{i}\" for i in range(1, n + 1)]\n",
    "    y_df = (\n",
    "        y_df.groupby(\"unique_id\")\n",
    "        .filter(lambda x: x.unique_id.iloc[0] in _ids)\n",
    "        .groupby(\"unique_id\")\n",
    "        .apply(transform_group)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return y_df\n",
    "\n",
    "\n",
    "def transform_group(df):\n",
    "    unique_id = df.unique_id.iloc[0]\n",
    "    _cnt = 60  # df.count()[0]\n",
    "    _start = pd.Timestamp(\"2018-01-01\")\n",
    "    _end = _start + pd.DateOffset(months=_cnt)\n",
    "    date_idx = pd.date_range(start=_start, end=_end, freq=\"M\", name=\"date\")\n",
    "    _df = (\n",
    "        pd.DataFrame(data=[], index=date_idx)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"date\"})\n",
    "    )\n",
    "    _df[\"unique_id\"] = unique_id\n",
    "    _df[\"y\"] = df[:60].y.values\n",
    "    return _df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "654114b5-2c2d-4b94-963f-eb18d5d06481",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We are going to save this data in a delta lake table. Provide catalog and database names where you want to store the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e959e6ea-ab90-45b6-ba97-f8a3f576cb3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"mmf\"  # Name of the catalog we use to manage our assets\n",
    "db = \"m4\"  # Name of the schema we use to manage our assets (e.g. datasets)\n",
    "user = spark.sql('select current_user() as user').collect()[0]['user'] # User email address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e66775a-c166-4217-bf24-e8d232078de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Making sure that the catalog and the schema exist\n",
    "_ = spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "_ = spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{db}\")\n",
    "\n",
    "(\n",
    "    spark.createDataFrame(create_m4_monthly())\n",
    "    .write.format(\"delta\").mode(\"overwrite\")\n",
    "    .saveAsTable(f\"{catalog}.{db}.m4_monthly_train\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "739a944c-afc9-45cc-92f1-5bce186dcb68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's take a peak at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af5a736-dbf3-40a4-8c49-df88194f816f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  spark.sql(f\"select unique_id, count(date) as count from {catalog}.{db}.m4_monthly_train group by unique_id order by unique_id\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dc1e94c-5c75-4a26-a090-138cc0f833c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  spark.sql(f\"select * from {catalog}.{db}.m4_monthly_train where unique_id in ('M1', 'M2', 'M3', 'M4', 'M5') order by unique_id, date\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d691a4cb-a985-4399-af34-6adeea8ed4b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the current value of shuffle partitions\n",
    "current = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "\n",
    "# If not set to 'auto' (serverless), convert to int; otherwise, use default 200\n",
    "if current != \"auto\":\n",
    "    current_val = int(current)\n",
    "else:\n",
    "    current_val = 200       \n",
    "\n",
    "# If n is greater than the current value, update the shuffle partitions setting\n",
    "if n > current_val:            \n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", str(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bcde0f5-c479-4af9-b898-55ff775578cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note that monthly forecasting requires the timestamp column to represent the last day of each month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b1e0013-28a8-4b76-9992-a956eec9911b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Models\n",
    "Let's configure a list of models we are going to apply to our time series for evaluation and forecasting. A comprehensive list of all supported models is available in [mmf_sa/models/README.md](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/mmf_sa/models/README.md). Look for the models where `model_type: local`; these are the local models we import from [statsforecast](https://github.com/Nixtla/statsforecast) and [sktime](https://github.com/sktime/sktime). Check their documentations for the description of each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d2ad93e-c75b-4d8b-9964-6533b7721f15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "active_models = [\n",
    "    \"StatsForecastBaselineWindowAverage\",\n",
    "    \"StatsForecastBaselineSeasonalWindowAverage\",\n",
    "    \"StatsForecastBaselineNaive\",\n",
    "    \"StatsForecastBaselineSeasonalNaive\",\n",
    "    \"StatsForecastAutoArima\",\n",
    "    \"StatsForecastAutoETS\",\n",
    "    \"StatsForecastAutoCES\",\n",
    "    \"StatsForecastAutoTheta\",\n",
    "    \"StatsForecastAutoTbats\",\n",
    "    \"StatsForecastAutoMfles\",\n",
    "    \"StatsForecastTSB\",\n",
    "    \"StatsForecastADIDA\",\n",
    "    \"StatsForecastIMAPA\",\n",
    "    \"StatsForecastCrostonClassic\",\n",
    "    \"StatsForecastCrostonOptimized\",\n",
    "    \"StatsForecastCrostonSBA\",\n",
    "    \"SKTimeProphet\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c2b819f-8ca1-47f2-bddf-1bcf764a36c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run MMF\n",
    "\n",
    "Now, we can run the evaluation and forecasting using `run_forecast` function defined in [mmf_sa/models/__init__.py](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/mmf_sa/models/__init__.py). Make sure to set `freq=\"M\"` in `run_forecast` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f693c738-bf1e-4d97-9a25-75bb19c6bb94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_forecast(\n",
    "    spark=spark,\n",
    "    train_data=f\"{catalog}.{db}.m4_monthly_train\",\n",
    "    scoring_data=f\"{catalog}.{db}.m4_monthly_train\",\n",
    "    scoring_output=f\"{catalog}.{db}.monthly_scoring_output\",\n",
    "    evaluation_output=f\"{catalog}.{db}.monthly_evaluation_output\",\n",
    "    group_id=\"unique_id\",\n",
    "    date_col=\"date\",\n",
    "    target=\"y\",\n",
    "    freq=\"M\",\n",
    "    prediction_length=3,\n",
    "    backtest_length=12,\n",
    "    stride=1,\n",
    "    metric=\"smape\",\n",
    "    train_predict_ratio=1,\n",
    "    data_quality_check=True,\n",
    "    resample=False,\n",
    "    active_models=active_models,\n",
    "    experiment_path=f\"/Users/{user}/mmf/m4_monthly\",\n",
    "    use_case_name=\"m4_monthly\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "662ab99a-c1dd-4460-b154-b67a51318b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluate\n",
    "In `evaluation_output` table, the we store all evaluation results for all backtesting trials from all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55906904-37d1-43e9-97ae-86fdcf601bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  spark.sql(f\"\"\"\n",
    "    select * from {catalog}.{db}.monthly_evaluation_output \n",
    "    where unique_id = 'M1'\n",
    "    order by unique_id, model, backtest_window_start_date\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db9e92f6-12c7-4a80-9b1b-3e7f4c3f97af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Forecast\n",
    "In `scoring_output` table, forecasts for each time series from each model are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e8c366b-df40-4d30-86a4-28bdffddc47c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "    select * from {catalog}.{db}.monthly_scoring_output \n",
    "    where unique_id = 'M1'\n",
    "    order by unique_id, model, date\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff65447-2337-4c55-85b7-0fd4134f37f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Refer to the [notebook](https://github.com/databricks-industry-solutions/many-model-forecasting/blob/main/examples/post-evaluation-analysis.ipynb) for guidance on performing fine-grained model selection after running `run_forecast`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9673b5f6-172f-4e04-846d-d5280c8917d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Delete Tables\n",
    "Let's clean up the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60ce814b-698a-4f4e-92b1-bde62f49964b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(spark.sql(f\"delete from {catalog}.{db}.monthly_evaluation_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d242c24-03f2-4dba-8291-388a16d9c30d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(spark.sql(f\"delete from {catalog}.{db}.monthly_scoring_output\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "/Workspace/Users/ryuta.yoshimatsu@databricks.com/many-model-forecasting"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "local_univariate_monthly",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
